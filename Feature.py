# -*- coding: utf-8 -*-
"""Review2Phase4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11n-mERb5EoZr0ozX1tAP05rzZPpoW1tL
"""

from urllib.parse import urlparse,urlencode
import ipaddress
import re
def havingIP(url):
    match = re.search(
        '(([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.'
        '([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\/)|'  # IPv4
        '((0x[0-9a-fA-F]{1,2})\\.(0x[0-9a-fA-F]{1,2})\\.(0x[0-9a-fA-F]{1,2})\\.(0x[0-9a-fA-F]{1,2})\\/)|'  # IPv4 in hexadecimal
        '(?:[a-fA-F0-9]{1,4}:){7}[a-fA-F0-9]{1,4}|'
        '[0-9a-fA-F]{7}', url)  # Ipv6
    if match:
        return 1
    else:
        return 0

def haveAtSign(url):
  if "@" in url:
     return 1            # phishing
  else:
     return 0            # legitimate

def getLength(url):
    if len(url) < 54:
        length = 1
    else:
        length = 0
    return length

from urllib.parse import urlparse
def getDepth(url):
    parsed_url = urlparse(url)
    path = parsed_url.path
    segments = path.split('/')
    depth = sum(1 for segment in segments if segment)
    return depth

def redirection(url):
    pos = url.rfind('//')
    if pos > 6:
        if pos > 7:
            return 1
        else:
            return 0
    else:
        return 0

from urllib.parse import urlparse

def httpDomain(url):
    parsed_url = urlparse(url)
    if parsed_url.scheme == 'https':
        return 1
    else:
        return 0

shortening_services = r"bit\.ly|goo\.gl|shorte\.st|go2l\.ink|x\.co|ow\.ly|t\.co|tinyurl|tr\.im|is\.gd|cli\.gs|" \
                      r"yfrog\.com|migre\.me|ff\.im|tiny\.cc|url4\.eu|twit\.ac|su\.pr|twurl\.nl|snipurl\.com|" \
                      r"short\.to|BudURL\.com|ping\.fm|post\.ly|Just\.as|bkite\.com|snipr\.com|fic\.kr|loopt\.us|" \
                      r"doiop\.com|short\.ie|kl\.am|wp\.me|rubyurl\.com|om\.ly|to\.ly|bit\.do|t\.co|lnkd\.in|db\.tt|" \
                      r"qr\.ae|adf\.ly|goo\.gl|bitly\.com|cur\.lv|tinyurl\.com|ow\.ly|bit\.ly|ity\.im|q\.gs|is\.gd|" \
                      r"po\.st|bc\.vc|twitthis\.com|u\.to|j\.mp|buzurl\.com|cutt\.us|u\.bb|yourls\.org|x\.co|" \
                      r"prettylinkpro\.com|scrnch\.me|filoops\.info|vzturl\.com|qr\.net|1url\.com|tweez\.me|v\.gd|" \
                      r"tr\.im|link\.zip\.net"
def tinyURL(url):
    match=re.search(shortening_services,url)
    if match:
        return 1
    else:
        return 0

def prefixSuffix(url):
    if '-' in urlparse(url).netloc:
        return 1
    else:
        return 0

import re
from bs4 import BeautifulSoup
import whois
"""!pip install urllib"""
import urllib
import urllib.request
from datetime import datetime

def web_traffic(url):
    """try:
        querystring = {"domain": url}
        headers = {
        "X-RapidAPI-Key": "cd4733fedbmsh6f2cfc21cf195f2p1d088djsn84e6c824c74e",
        "X-RapidAPI-Host": "similar-web.p.rapidapi.com"
        }
        response = requests.get("https://similar-web.p.rapidapi.com/get-analysis", headers=headers, params=querystring)
        print("Response Status Code:", response.status_code)  # Print status code
        data = response.json()
        rank = data['GlobalRank']['Rank']
        rank = int(rank)
    except (requests.exceptions.RequestException, ValueError, KeyError):
        rank = 1
    if rank < 100000:
        return 1
    else:
        return 0"""
    try:
        if url.startswith("http://"):
            return 1
        else:
            return 0
    except Exception as e:
        print("Error:", e)
        return None
    
import requests

def iframe(response):
    if response == "":
        return 1
    else:
        if re.findall(r"[<iframe>|<frameBorder>]", response.text):
            return 0
        else:
            return 1

def mouseOver(response):
    if response == "" :
        return 1
    else:
        if re.findall("<script>.+onmouseover.+</script>", response.text):
            return 0
        else:
            return 1

def rightClick(response):
    if response == "":
        return 1
    else:
        if re.findall(r"event.button ?== ?2", response.text):
            return 0
        else:
            return 1

def forwarding(response):
    if response == "":
        return 1
    else:
        if len(response.history) <= 2:
            return 0
        else:
            return 1

import pickle

with open('RandomForestClassifier.pickle.dat', 'rb') as f:
    model = pickle.load(f)

import pickle
import whois
import requests
from urllib.parse import urlparse

feature_names = [
    "Having IP",
    "Having @ sign",
    "URL Length",
    "URL Depth",
    "Redirection",
    "HTTP Domain",
    "TinyURL",
    "Prefix/Suffix",
    "DNS",
    "Web Traffic",
    "Domain Age",
    "Domain Expiry",
    "IFrame",
    "Mouse Over",
    "Right Click",
    "Forwarding"
]

# Function to extract features from a URL
def featureExtraction(url):
    features = []
    features.append(havingIP(url))
    features.append(haveAtSign(url))
    features.append(getLength(url))
    features.append(getDepth(url))
    features.append(redirection(url))
    features.append(httpDomain(url))
    features.append(tinyURL(url))
    features.append(prefixSuffix(url))

    dns = 0
    dns_age = 0
    dns_end = 0
    features.append(dns)
    features.append(dns_age)
    features.append(dns_end)
    features.append(web_traffic(url))
    try:
        response = requests.get(url)
    except:
        response = ""
    features.append(iframe(response))
    features.append(mouseOver(response))
    features.append(rightClick(response))
    features.append(forwarding(response))

    return features
#import pickle
"""""
# Load the RandomForestClassifier model
with open('RandomForestClassifier.pickle.dat', 'rb') as f:
    rf_model = pickle.load(f)

# Load the XGBoostClassifier model
with open('XGBoostClassifier.pickle.dat', 'rb') as f:
    xgb_model = pickle.load(f)

# Function to predict label using both models
def predict_label(url, rf_model, xgb_model):
    # Extract features from the URL
    features = featureExtraction(url)

    # Print the extracted features with names
    print("Extracted Features:")
    for name, value in zip(feature_names, features):
        print(f"{name}: {value}")

    # Use the RandomForestClassifier model to predict the label based on the features
    rf_prediction = rf_model.predict([features])[0]

    # Use the XGBoostClassifier model to predict the label based on the features
    xgb_prediction = xgb_model.predict([features])[0]

    # Classify the URL based on the predictions of both models
    if rf_prediction == 1 or xgb_prediction == 1:
        label = "phishing"
    elif rf_prediction == 0 and xgb_prediction == 0:
        label = "legitimate"
    else:
        label = "suspicious"

    return label

# Get URL input from the user
url = input("Enter the URL: ")

# Predict the label for the provided URL using both models
predicted_label = predict_label(url, rf_model, xgb_model)

# Print the predicted label
print("Predicted Label:", predicted_label)"""